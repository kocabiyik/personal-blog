---
title: Visualizing Machine Learning Optimizers
author: Imran Kocabiyik
date: '2021-05-07'
slug: visualizing-ml-optimizers
categories:
  - Machine Learning
tags:
  - plot
topic: Deep Learning
---



<p>Having a strong intuition about algorithms are helpful for hacking and quick prototyping. For me, the most useful tools for grasping good intuition are visualizations and analogies.</p>
<p><a href="https://www.deeplearningbook.org/">The Deep Learning book</a> has <a href="https://www.deeplearningbook.org/contents/optimization.html">a comprehensive chapter</a> on optimizers. I am currently reading it and practicing them by implementing them into code. I am sharing my visualizations here so that it might be helpful for other learners.</p>
<div id="basic-optimization-algorithms" class="section level2">
<h2>Basic Optimization Algorithms</h2>
<p>What is an optimizer? It is how we are updating the parameters. The objective is finding a set of <span class="math inline">\(\theta\)</span> values so that <span class="math inline">\(J(\theta)\)</span> is significantly lower. Starting with a relatively simple one:</p>
<div id="stochastic-gradient-descent" class="section level3">
<h3>Stochastic Gradient Descent</h3>
<p>The most critical parameter is the learning rate. The higher the learning rate is, the higher the step size taken.</p>
<p><strong>Implementation:</strong></p>
<ul>
<li><span class="math inline">\(\theta\)</span> is the parameter to learn.<br />
</li>
<li><span class="math inline">\(\epsilon\)</span> is the learning rate. a hyperparameter.</li>
</ul>
<p>do iteratively:<br />
1. Compute gradients and set it to <span class="math inline">\(g\)</span>.<br />
3. Apply update: <span class="math inline">\(\theta \leftarrow \theta - \epsilon g\)</span></p>
<p>Below is the update steps with a fixed learning rate:</p>
<p><img src="images/sgd.gif" /></p>
<p>One way of making the learning faster is Decaying the Learning Rate:</p>
<p><strong>Implementation:</strong></p>
<ul>
<li><p>The learning rate is decayed until the iteration <span class="math inline">\(\tau\)</span>.</p></li>
<li><p>The learning rate on the iteration <span class="math inline">\(k\)</span> calculated as <span class="math inline">\(\epsilon_k=(1-\alpha)\epsilon_0+\alpha \epsilon_{\tau}\)</span> where <span class="math inline">\(\alpha = \frac{k}{\tau}\)</span></p></li>
<li><p>The learning rate after the iteration <span class="math inline">\(\tau\)</span> is kept constant.<br />
</p></li>
<li><p><span class="math inline">\(\epsilon_{\tau}\)</span> is generally set to 1 % of the initial learning rate (<span class="math inline">\(\epsilon_0\)</span>).</p></li>
</ul>
<p>Below animation is the update steps with a decaying learning rate applied. The initial learning rate is set to a higher value than the one in the previous example, but it is decayed after some iteration.</p>
<p><img src="images/sgd-with-lr-decay.gif" /></p>
</div>
<div id="momentum" class="section level3">
<h3>Momentum</h3>
<p>Momentum is one technique for accelerating learning.</p>
<p>Momentum requires a new hyperparameter: <span class="math inline">\(\alpha\)</span> which controls the acceleration. It is a common practice to pick a value of 0.9, 0.95, or 0.99. It doesn’t have to be a fixed value but can be adapted in the training process.</p>
<p><strong>Implementation:</strong></p>
<ul>
<li><span class="math inline">\(\theta\)</span> is the parameter to learn.<br />
</li>
<li><span class="math inline">\(\epsilon\)</span> is the learning rate. a hyperparameter.<br />
</li>
<li><span class="math inline">\(\alpha\)</span> controls the
ration. a hyperparameter.<br />
</li>
<li><span class="math inline">\(v\)</span> is the velocity and it is initially set to 0. a hyperparameter.</li>
</ul>
<p>do iteratively:<br />
1. Compute gradients and set it to <span class="math inline">\(g\)</span> .<br />
2. Compute velocity update as <span class="math inline">\(v \leftarrow \alpha v − \epsilon g\)</span><br />
3. Apply update: <span class="math inline">\(\theta \leftarrow \theta + v\)</span></p>
<p>Here is an animation in a 3D surface:</p>
<p><img src="images/sgd-with-momentum.gif" /></p>
<p>The problem of oscillations:<br />
Applying momentum can result in too</p>
<p>oscillations. As you can see in the above illustrations, there many U-turns and spirals around local/global minimum points. Nesterov Momentum is reducing those oscillations.</p>
</div>
<div id="nesterov-momentum" class="section level3">
<h3>Nesterov Momentum</h3>
<p>The difference between the standard Momentum and the Nesterov momentum algorithms is <em>where</em> the gradients are calculated.<br />
In the Nesterov Momentum, gradients are evaluated after the current velocity is applied.</p>
<p><strong>Implementation:</strong></p>
<ul>
<li><span class="math inline">\(\theta\)</span> is the parameter to learn.<br />
</li>
<li><span class="math inline">\(\epsilon\)</span> is the learning rate. a hyperparameter.<br />
</li>
<li><span class="math inline">\(\alpha\)</span> controls the acceleration. a hyperparameter.<br />
</li>
<li><span class="math inline">\(v\)</span> is the velocity and it is initially set to 0. a hyperparameter.</li>
</ul>
<p>do iteratively:<br />
1. Apply and interim update <span class="math inline">\(\tilde{\theta} \leftarrow \theta + \alpha v\)</span><br />
2. Compute gradients (at interim point) with parameter <span class="math inline">\(\tilde{\theta}\)</span> and set it to <span class="math inline">\(g\)</span>.<br />
3. Compute velocity update as <span class="math inline">\(v \leftarrow \alpha v − \epsilon g\)</span><br />
4. Apply update: <span class="math inline">\(\theta \leftarrow \theta + v\)</span></p>
<p>Below is a visualization of the standard momentum (black) vs Nesterov momentum (red).<br />
The Nesterov Momentum is not oscilating much comparing to the standard Momentum algorithm.</p>
<p><img src="images/momentum-vs-nesterov-momentum.gif" /></p>
</div>
</div>
<div id="optimizers-with-adaptive-learning-rates" class="section level2">
<h2>Optimizers with Adaptive Learning Rates</h2>
<div id="adagrad" class="section level3">
<h3>AdaGrad</h3>
<p>It is simple. We keep putting on the breaks on the descending hills, and we don’t put too many on the plateaus.</p>
<p>Technically speaking:</p>
<ul>
<li>A small decrease in the learning rate when the partial derivative is small.<br />
</li>
<li>A rapid decrease in the learning rate when the partial derivative is large.</li>
</ul>
<p>One issue:<br />
The accumulation of the gradients <em>from the beginning</em> is resulting in an excessive decrease in the learning rate.<br />
It works fine in a convex function, but it might be stuck in local minimums in non-convex settings.<br />
The next algorithm, RMSProp will handle this issue.</p>
<p><strong>Implementation:</strong></p>
<ul>
<li><span class="math inline">\(\theta\)</span> is the parameter to learn.<br />
</li>
<li><span class="math inline">\(\epsilon\)</span> is the learning rate. a hyperparameter.<br />
</li>
<li><span class="math inline">\(\delta\)</span> small constant for numerical stability (for avoiding division by zero). a hyperparameter.<br />
</li>
<li><span class="math inline">\(r\)</span> gradient accumulation variable. Initially 0.</li>
</ul>
<p>do iteratively:<br />
1. Compute gradients and set it to <span class="math inline">\(g\)</span>.<br />
2. Accumulate squared gradient: <span class="math inline">\(r \leftarrow r + g \odot g\)</span><br />
3. Compute update: <span class="math inline">\(\Delta \boldsymbol{\theta} \leftarrow-\frac{\epsilon}{\delta+\sqrt{\boldsymbol{r}}} \odot \boldsymbol{g}\)</span><br />
4. Apply Update: <span class="math inline">\(\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\Delta \boldsymbol{\theta}\)</span></p>
</div>
<div id="rmsprop" class="section level3">
<h3>RMSProp</h3>
<p>RMSProp is not taking the entire history but the recent ones. Here is a comparison of AdaGrad (gray) and RMSProp (red):</p>
<p><strong>Implementation:</strong></p>
<ul>
<li><span class="math inline">\(\theta\)</span> is the parameter to learn.<br />
</li>
<li><span class="math inline">\(\epsilon\)</span> is the learning rate. a hyperparameter.<br />
</li>
<li><span class="math inline">\(\rho\)</span> is the decay rate.<br />
</li>
<li><span class="math inline">\(\delta\)</span> small constant for numerical stability (for avoiding division by zero). a hyperparameter.<br />
</li>
<li><span class="math inline">\(r\)</span> gradient accumulation variable. Initially 0.</li>
</ul>
<p>do iteratively:<br />
1. Compute gradients and set it to <span class="math inline">\(g\)</span>.<br />
2. Accumulate squared gradient: <span class="math inline">\(r \leftarrow \rho r+(1-\rho) g \odot g\)</span><br />
3. Compute parameter update: <span class="math inline">\(\Delta \boldsymbol{\theta}=-\frac{\epsilon}{\sqrt{\delta+\boldsymbol{r}}} \odot \boldsymbol{g}\)</span><br />
4. Apply Update: <span class="math inline">\(\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\Delta \boldsymbol{\theta}\)</span></p>
<p><img src="images/adagrad-vs-rmsprop.gif" /></p>
</div>
<div id="adam" class="section level3">
<h3>Adam</h3>
<p>And, finally Adam. More or less, it covers most of the ideas the previous algorithms have to offer. It is my default choice in training machine learning models.</p>
<p><strong>Implementation:</strong></p>
<ul>
<li><span class="math inline">\(\theta\)</span> is the parameter to learn.<br />
</li>
<li><span class="math inline">\(\delta\)</span> small constant for numerical stability (for avoiding division by zero). a hyperparameter.<br />
</li>
<li><span class="math inline">\(\epsilon\)</span> is the learning rate. a hyperparameter.<br />
</li>
<li><span class="math inline">\(\rho1\)</span> and <span class="math inline">\(\rho1\)</span> are the decay rates.<br />
</li>
<li><span class="math inline">\(s\)</span> and <span class="math inline">\(r\)</span> are the first and the second variables. Initially both of them are 0.</li>
</ul>
<p>do iteratively:<br />
1. Compute gradients and set it to <span class="math inline">\(g\)</span>.<br />
2. <span class="math inline">\(t \leftarrow t+1\)</span><br />
3. Update biased ﬁrst moment estimate: <span class="math inline">\(s \leftarrow \rho_{1} s+\left(1-\rho_{1}\right) \boldsymbol{g}\)</span><br />
4. Update biased second moment estimate: <span class="math inline">\(\boldsymbol{r} \leftarrow \rho_{2} \boldsymbol{r}+\left(1-\rho_{2}\right) \boldsymbol{g} \odot \boldsymbol{g}\)</span><br />
5. Correct bias in ﬁrst moment: <span class="math inline">\(\hat{\boldsymbol{s}} \leftarrow \frac{\boldsymbol{s}}{1-\rho_{1}^{t}}\)</span><br />
6. Correct bias in second moment: <span class="math inline">\(\hat{\boldsymbol{r}} \leftarrow \frac{\boldsymbol{r}}{1-\rho_{2}^{t}}\)</span><br />
7. Compute Update: <span class="math inline">\(\Delta \boldsymbol{\theta}=-\epsilon \frac{\hat{\boldsymbol{s}}}{\sqrt{\hat{\boldsymbol{r}}}+\delta}\)</span><br />
8. Apply Update: <span class="math inline">\(\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\Delta \boldsymbol{\theta}\)</span></p>
<p>Here is an animation. Moving like a snake:</p>
<p><img src="images/adam.gif" /></p>
</div>
</div>
